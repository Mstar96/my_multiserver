### 2025/8/19
## 目前情况
```
thread_task_list = [162, 433, 189]
fi_funcs = [0.22, 0.72, 0.44]
Train DQN-LL:  10%|██████                                                       | 1995/20000 [00:46<07:34, 39.62it/s]EP 2000: DQN_Time=113.6951, MRASS_Time=89.2843, MRASS_Allocation=[15, 9, 6], DQN_Allocation=[ 5 13 12]
Train DQN-LL:  20%|████████████▏                                                | 3997/20000 [01:45<06:33, 40.65it/s]EP 4000: DQN_Time=189000000.0000, MRASS_Time=89.2843, MRASS_Allocation=[15, 9, 6], DQN_Allocation=[14 16  0]
Train DQN-LL:  30%|██████████████████▎                                          | 5999/20000 [02:34<05:27, 42.80it/s]EP 6000: DQN_Time=162000000.0000, MRASS_Time=89.2843, MRASS_Allocation=[15, 9, 6], DQN_Allocation=[ 0 28  2]
Train DQN-LL:  40%|████████████████████████▍                                    | 7996/20000 [03:21<04:20, 46.09it/s]EP 8000: DQN_Time=189000000.0000, MRASS_Time=89.2843, MRASS_Allocation=[15, 9, 6], DQN_Allocation=[ 0 30  0]
Train DQN-LL:  50%|██████████████████████████████▍                              | 9995/20000 [04:08<03:55, 42.43it/s]EP 10000: DQN_Time=189000000.0000, MRASS_Time=89.2843, MRASS_Allocation=[15, 9, 6], DQN_Allocation=[ 0 30  0]
Train DQN-LL:  60%|███████████████████████████████████▉                        | 11997/20000 [04:56<03:15, 40.95it/s]EP 12000: DQN_Time=162000000.0000, MRASS_Time=89.2843, MRASS_Allocation=[15, 9, 6], DQN_Allocation=[ 0 29  1]
Train DQN-LL:  70%|█████████████████████████████████████████▉                  | 13997/20000 [05:43<02:27, 40.71it/s]EP 14000: DQN_Time=162000000.0000, MRASS_Time=89.2843, MRASS_Allocation=[15, 9, 6], DQN_Allocation=[ 0 28  2]
Train DQN-LL:  80%|███████████████████████████████████████████████▉            | 15996/20000 [06:31<01:32, 43.36it/s]EP 16000: DQN_Time=162000000.0000, MRASS_Time=89.2843, MRASS_Allocation=[15, 9, 6], DQN_Allocation=[ 0 28  2]
Train DQN-LL:  90%|█████████████████████████████████████████████████████▉      | 17996/20000 [07:18<00:48, 40.98it/s]EP 18000: DQN_Time=162000000.0000, MRASS_Time=89.2843, MRASS_Allocation=[15, 9, 6], DQN_Allocation=[ 0 29  1]
Train DQN-LL: 100%|███████████████████████████████████████████████████████████▉| 19998/20000 [08:05<00:00, 45.69it/s]EP 20000: DQN_Time=162000000.0000, MRASS_Time=89.2843, MRASS_Allocation=[15, 9, 6], DQN_Allocation=[ 0 27  3]
Train DQN-LL: 100%|████████████████████████████████████████████████████████████| 20000/20000 [08:05<00:00, 41.16it/s]
Final: DQN_time=162000000.0000, DQN_Allocation=[ 0 27  3], MRASS_Time=89.2843, MRASS_alloc=[15, 9, 6]
```
- DQN 一开始还能输出合理分配，但后面完全崩坏：
- 输出类似 [0, 30, 0]、[0, 29, 1]
- 完成时间直接变成 1.62e8，说明 RL 完全没学到  

**可能原因**
1. 奖励函数设计不合理  
直接用 -完成时间 作为 reward，但完成时间的数值跨度太大（几十 ~ 上亿），reward 极端不稳定，导致 Q 网络梯度爆炸/发散。  
2. 动作空间过于稀疏  
动作是「给哪个线程分配 1 单位资源」，这种 逐步加资源 的环境，奖励只有在最后分配完成才有意义（延迟奖励）。如果 reward 只有 done=True 时才给，训练会非常困难。
3. epsilon 衰减太快  
设的是 eps_decay_steps = 2000，20k 回合里前 2k 就从 1.0 衰减到 0.05，探索不足，基本没怎么见过合理分配。
4. reward 没做归一化  
DQN 对 reward 尺度非常敏感，reward 范围跨度太大，容易让网络输出 saturate。

**改进**  

1. 在环境里修改 Reward
别直接用完成时间，可以设计为 增量奖励 或 归一化奖励：  
```
reward = (baseline_time - complete_time) / baseline_time
```
baseline_time 可以用 MRASS 算的结果，或者 单线程最慢完成时间。

这样 reward 范围在 [-1, 1] 左右，训练更稳定

也可以用「资源分配边际增益」作为 step reward：  
~~~
reward = prev_time - new_time
~~~
每次分配 1 单位资源时，奖励就是完成时间减少了多少。
这样 agent 每一步都有正反馈，更容易学  

2. 调整探索策略
改大eps_decay_steps，比如 eps_decay_steps=20000（和总回合一样长），保证足够探索。

或者用 ε-greedy + Boltzmann exploration。

3. 归一化reward
(prev_time - cur_time) / baseline_time 的范围一般在 [-1, 1]，不会爆炸

4. 多输出一下平均reward，确保模型确实在学

### 改动要点
1. 中间奖励  
归一化边际改善 reward = (old_M - new_M) / old_M
奖励值稳定在 [-1,1] 内。

2. 最终奖励  
额外比较 RL 分配 vs MRASS baseline：  
RL 更好 → 额外正奖励  
RL 更差 → 额外负奖励